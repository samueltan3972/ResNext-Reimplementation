# -*- coding: utf-8 -*-
"""Pikachu - ResNext Deep Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8Izbi_YAAesav1f1x4PyB-VGhhN0CDx
"""

# from google.colab import drive
# drive.mount('/content/drive')

# cd "/content/drive/MyDrive/pikachu_dataset/"

import numpy as np
import matplotlib.pyplot as plt
import os

import torch, torchvision
import torch.nn as nn
import torch.nn.functional as F

import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.optim import lr_scheduler

import math
import json

if not os.path.exists("./models"):
      os.mkdir("models")

device = "cuda" if torch.cuda.is_available() else "cpu"

"""**Helper Function**"""

def train(model, trainloader, num_epoch=5, dataset_desc='', val_loader=None, lr=0.1, momentum=0.9, weight_decay=0.001, savedModel=False):
    
    # to store loss and accuracy stat
    model_stats = {}

    accuracy_stats = {
        'train': [],
        "val": []
    }

    loss_stats = {
        'train': [],
        "val": []
    }

    # compute loss 3 times in each epoch
    loss_iterations = int(np.ceil(len(trainloader)/3))
    stagnant_count = 0
    best_loss = math.inf

    # transfer model to GPU
    model = model.to(device)

    # set the optimizer
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)

    # set the scheduler
    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    # init first epoch
    previous_epoch = -1

    # prepare to load checkpoint file
    if savedModel:
      path = './models/' + dataset_desc

      # load the checkpoint file
      if not os.path.exists(path):
        os.mkdir(path)

      model_files = [ f  for f in os.listdir(path) if f.endswith('.pt')]
      model_files.sort(reverse=True)

      # load if only exist checkpoint file
      if model_files:
        try: 
          checkpoint = torch.load(path + '/' + model_files[0])      
          model.load_state_dict(checkpoint['model_state_dict'])
          optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
          previous_epoch = checkpoint['epoch']
          previous_train_loss = checkpoint['train_loss']
          previous_val_loss = checkpoint['val_loss']

          print(f'Resuming previous epoch. Last run epoch: {previous_epoch}, last train loss: {previous_train_loss}, last val loss: {previous_val_loss}')
        except: 
          print("Error in saved model")    
      
      # load the savedModel stat file
      stat_file_name = path + '/' + dataset_desc + '_stat.json'
      
      if os.path.isfile(stat_file_name):
        with open(stat_file_name, 'r') as f:
          try:
            model_stats = json.load(f)
            accuracy_stats = model_stats["accuracy"]
            loss_stats = model_stats["loss"]
          except:
            print("Error in saved model stat")   

    # train the network
    for e in range(previous_epoch+1, previous_epoch+1+num_epoch):    

        # set to training mode
        model.train()

        running_train_loss = 0
        running_train_count = 0
        running_train_corrects = 0
        
        for i, (train_inputs, train_labels) in enumerate(trainloader):
            # Clear all the gradient to 0
            optimizer.zero_grad()

            # transfer data to GPU
            train_inputs = train_inputs.to(device)
            train_labels = train_labels.to(device)

            # forward propagation to get h
            train_outs = model(train_inputs)

            # compute train loss 
            train_loss = F.cross_entropy(train_outs, train_labels)

            # compute train accuracy
            _, predicted = torch.max(train_outs, 1)
            running_train_corrects += (train_labels == predicted).double().sum()

            # backpropagation to get gradients of all parameters
            train_loss.backward()

            # update parameters
            optimizer.step()

            # get the loss
            running_train_loss += train_loss.item()
            running_train_count += 1
            
            # display the averaged loss value (training)
            if i % loss_iterations == loss_iterations-1 or i == len(trainloader) - 1:    
                # compute training loss
                train_loss = running_train_loss / running_train_count
                running_train_loss = 0. 
                running_train_count = 0.
               
                print(f'[Epoch {e+1:2d} Iter {i+1:5d}/{len(trainloader)}]: train_loss = {train_loss:.4f}')       

                # track train loss
                loss_stats['train'].append(train_loss)

                # to stop the training if the result is not improving
                # if train_loss >= best_loss:
                #   stagnant_count += 1
                # else:
                #   best_loss = train_loss
                #   stagnant_count = 0

                # if stagnant_count >= 3:
                #   break
              
        # track train accuracy
        train_accuracy = 100*running_train_corrects/len(trainloader.dataset)
        accuracy_stats['train'].append(train_accuracy.item())
        
        # print train accuracy
        print(f'[Epoch {e+1:2d}: train_accuracy = {train_accuracy:.2f}%')   

        # init val loss
        val_loss = math.inf

        # VALIDATION
        if val_loader is not None:
          with torch.no_grad():

              # set to evaluation mode
              model.eval()

              running_val_loss = 0
              running_val_count = 0
              running_val_corrects = 0

              for i, (val_inputs, val_labels) in enumerate(val_loader):

                  # transfer data to GPU
                  val_inputs = val_inputs.to(device)
                  val_labels = val_labels.to(device)

                  # prediction with validation set
                  val_outs = model(val_inputs)

                  # compute loss 
                  val_loss = F.cross_entropy(val_outs, val_labels)

                  # compute validation accuracy
                  _, predicted = torch.max(val_outs, 1)
                  running_val_corrects += (val_labels == predicted).double().sum()

                  # get loss
                  running_val_loss += val_loss.item()
                  running_val_count += 1
                  
                  # display the averaged loss value 
                  if i % loss_iterations == loss_iterations-1 or i == len(val_loader) - 1:    
                      # compute training loss
                      val_loss = running_val_loss / running_val_count
                      running_val_loss = 0. 
                      running_val_count = 0.
                    
                      print(f'[Epoch {e+1:2d} Iter {i+1:5d}/{len(val_loader)}]: val_loss = {val_loss:.4f}')       

                      # track validation loss
                      loss_stats['val'].append(val_loss)
                  
          # track validation accuracy
          val_accuracy = 100*running_val_corrects/len(val_loader.dataset)
          accuracy_stats['val'].append(val_accuracy.item())

          # print validation accuracy
          print(f'[Epoch {e+1:2d}: val_accuracy = {val_accuracy:.2f}%')   

        # Update the scheduler's counter at the end of each epoch
        scheduler.step()

        # stop the training if no improvement
        # if stagnant_count >= 3:
        #   return loss_stats, accuracy_stats

        # save the model 
        if savedModel:
          checkpoint_file = './models/' + dataset_desc + '/' + dataset_desc + '_model_epoch_'+ str(e) + '.pt'

          torch.save({
              'epoch': e,
              'train_loss': train_loss,
              'val_loss': val_loss,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict()
          }, checkpoint_file)

          # save the model stat
          stat_file_name = './models/' + dataset_desc + '/' + dataset_desc + '_stat.json'

          with open(stat_file_name, 'w') as f:
              model_stats["accuracy"] = accuracy_stats
              model_stats["loss"] = loss_stats
              json.dump(model_stats, f, indent=2)

    return loss_stats, accuracy_stats

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, RocCurveDisplay

def evaluate(net, testloader, printROC=False):
    
    # set to evaluation mode
    net.eval() 

    # running_correct
    running_corrects = 0
    y_pred = []
    y_true = []
    y_scores = []

    for inputs, targets in testloader:

        # transfer to the GPU
        inputs = inputs.to(device)
        targets = targets.to(device)

        # perform prediction (no need to compute gradient)
        with torch.no_grad():
            outputs = net(inputs)  
            _, predicted = torch.max(outputs, 1)

            running_corrects += (targets == predicted).double().sum()
            y_scores.extend(_.cpu().numpy())        # Save prediction scores
            y_pred.extend(predicted.cpu().numpy())  # Save prediction
            y_true.extend(targets.cpu().numpy())    # Save Truth
            
    print('Accuracy = {:.2f}%'.format(100*running_corrects/len(testloader.dataset)))

    # Plot Confusion Matrix
    conf_matrix = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=testloader.dataset.classes)
    disp.plot()
    plt.title('Confusion Matrix')
    plt.show() 

    # Plot ROC Curve and AUC value
    if printROC:
      fpr, tpr, thresholds = roc_curve(y_true, y_scores)
      roc_auc = auc(fpr, tpr)
      display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='ROC Curve')
      display.plot()
      plt.title('ROC Curve')
      plt.show()

"""**ResNext Model** 

Will be compare with resnext50 (reimplement), restnext34, restnext18 (new model) and ML algo (Random Forest, KNN, etc) and resnet (prebuilt)
"""

def conv1x1(input, output, stride=1):
  return nn.Conv2d(input, output, kernel_size=1, stride=stride, bias=False)

def conv3x3(input, output, groups=1, padding=0, stride=1):
  return nn.Conv2d(input, output, kernel_size=3, groups=groups, padding=padding, stride=stride, bias=False)

class Bottleneck(nn.Module):
  def __init__(self, in_channel, mid_channel, out_channel, stride=1, downsample = None):
    super().__init__()

    self.conv1 = conv1x1(in_channel, mid_channel)
    self.bn1 = nn.BatchNorm2d(mid_channel)
    self.conv2 = conv3x3(mid_channel, mid_channel, groups=32, padding=1, stride=stride)
    self.bn2 = nn.BatchNorm2d(mid_channel)
    self.conv3 = conv1x1(mid_channel, out_channel)
    self.bn3 = nn.BatchNorm2d(out_channel)
    self.relu = nn.ReLU(inplace=True)
    self.downsample = downsample

  def forward(self, x):
    identity = x

    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)

    out = self.conv2(out)
    out = self.bn2(out)
    out = self.relu(out)

    out = self.conv3(out)
    out = self.bn3(out)

    if self.downsample is not None:
      identity = self.downsample(x)

    out += identity  # skip connection
    out = self.relu(out)

    return out

# Resnext50 reimplement
class resnext50_reimplement(nn.Module):
  def __init__(self, num_classes=10):
    super().__init__()
    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, dilation=1)
    self.layer1 = self._make_layer(1)
    self.layer2 = self._make_layer(2)
    self.layer3 = self._make_layer(3)
    self.layer4 = self._make_layer(4)
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear(2048, num_classes)
    self.softmax = nn.Softmax(dim=1)

    for m in self.modules():
      if isinstance(m, nn.Conv2d):
          nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
      elif isinstance(m, (nn.BatchNorm2d)):
          nn.init.ones_(m.weight)
          nn.init.zeros_(m.bias)

  def _make_layer(self, layer_num):
    layer_sizes = [3, 4, 6, 3]
    channel_sizes = [(64, 128, 256), (256, 256, 512), (512, 512, 1024), (1024, 1024, 2048)]

    # init stride for layer
    if layer_num == 1:
      stride = 1
    else:
      stride = 2

    # to fit to list position
    layer_num -=  1

    # downsample layer for residual connection
    downsample = nn.Sequential(
      conv1x1(channel_sizes[layer_num][0], channel_sizes[layer_num][2], stride=stride),
      nn.BatchNorm2d(channel_sizes[layer_num][2])
    )
    
    # generate first layer in each layer blocks
    layers = []
    layers.append(
        Bottleneck(channel_sizes[layer_num][0], channel_sizes[layer_num][1], channel_sizes[layer_num][2], stride=stride, downsample=downsample)
    )

    # generate the rest of the layers
    for _ in range(1, layer_sizes[layer_num]):
      layers.append(
          Bottleneck(channel_sizes[layer_num][2], channel_sizes[layer_num][1], channel_sizes[layer_num][2])
      )

    # return the layer block
    return nn.Sequential(*layers)

  def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.maxpool(x)

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    x = self.layer4(x)

    x = self.avgpool(x)
    x = torch.flatten(x, 1)
    x = self.fc(x)

    #x = self.softmax(x)

    return x

# To experiment on why disabled bias on conv layer with batch norm
class Net(nn.Module):
  def __init__(self):
    super().__init__()
    self.relu = nn.ReLU(inplace=True)
    self.conv1 = nn.Conv2d(3, 256, kernel_size=3, bias=False)
    self.bn1 = nn.BatchNorm2d(256)

    self.conv2 = nn.Conv2d(256, 256, kernel_size=3, bias=False)
    self.bn2 = nn.BatchNorm2d(256)

    self.conv3 = nn.Conv2d(256, 512, kernel_size=3, bias=False)
    self.bn3 = nn.BatchNorm2d(512)

    self.conv4 = nn.Conv2d(512, 512, kernel_size=3, bias=False)
    self.bn4 = nn.BatchNorm2d(512)

    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear(512, 2)

  def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)

    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)

    x = self.conv3(x)
    x = self.bn3(x)
    x = self.relu(x)

    x = self.conv4(x)
    x = self.bn4(x)
    x = self.relu(x)
    
    x = self.avgpool(x)
    x = torch.flatten(x, 1)
    x = self.fc(x)
  
    return x

"""**Load Dataset**

Load CIFAR 10 to test and build the model first, custom dataset will be later
"""

from torch.utils.data import Dataset
from PIL import Image

class PikachuDataset(Dataset):

  def __init__(self, dataType='train', transform=None):
    self.data = []
    self.labels = []
    self.transform = transform
    self.classes = ['not_pikachu', 'pikachu']

    # get the training samples
    for class_id, cls in enumerate(self.classes):
      
      cls_folder = os.path.join('pikachu_dataset/' + dataType, cls)

      # get the training samples for the class 'cls'
      for img_name in os.listdir(cls_folder):
        self.data.append(os.path.join(cls_folder, img_name))
        self.labels.append(class_id)
  
  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):

    # get the image
    image = Image.open(self.data[idx]).convert('RGB')

    # perform transformation
    if self.transform is not None:
      image = self.transform(image)

    # get the label
    label = self.labels[idx]

    # return sample
    return image, label

transform = transforms.Compose([
 transforms.Resize((256, 256)),
 transforms.CenterCrop(224),
 transforms.RandomHorizontalFlip(),
 transforms.ToTensor(),
 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
pikachu_trainset = PikachuDataset('train', transform=transform)
pikachu_validationset = PikachuDataset('validation', transform=transform)
pikachu_testset = PikachuDataset('test', transform=transform)

print('Number of samples in dataset:', len(pikachu_trainset))
print('Number of classes:', pikachu_trainset.classes)
print('Number of samples in dataset:', len(pikachu_validationset))
print('Number of classes:', pikachu_validationset.classes)
print('Number of samples in dataset:', len(pikachu_testset))
print('Number of classes:', pikachu_testset.classes)

pikachu_trainloader = DataLoader(pikachu_trainset, batch_size=8, shuffle=True, num_workers=2)
pikachu_val_loader = DataLoader(pikachu_validationset, batch_size=8, shuffle=True, num_workers=2)
pikachu_testloader = DataLoader(pikachu_testset, batch_size=8, shuffle=True, num_workers=2)

"""**Init model**"""

# define a model
random_model = Net()

# define reimplement model
resnext50_reimplement_model = resnext50_reimplement(num_classes=2)

import torchvision.models as models

# build the network
resnext50_model = models.resnext50_32x4d()
in_c = resnext50_model.fc.in_features
resnext50_model.fc = nn.Linear(in_c, 2)

resnet18_model = models.resnet18()
in_c = resnet18_model.fc.in_features
resnet18_model.fc = nn.Linear(in_c, 2)

"""**TensorBoard**"""

from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/pikachu_model')

inputs =torch.randn(4, 3, 224, 224)
writer.add_graph(resnext50_reimplement_model, inputs)

writer.close()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir runs
#%reload_ext tensorboard

"""**Inspect model / View model**

Change the last fc layer to specific number
"""

print(resnext50_reimplement_model)

print(resnext50_model)

print(resnet18_model)

"""**Train model**"""

random_model_loss_history, random_model_acc_history = train(random_model, pikachu_trainloader, num_epoch=10, 
                                                            dataset_desc='random_model_pikachu_dataset', 
                                                            val_loader=pikachu_val_loader,
                                                            weight_decay=0.1, savedModel=True)

resnext50_reimplement_loss_history, resnext50_reimplement_acc_history = train(resnext50_reimplement_model, pikachu_trainloader, num_epoch=10, 
                                                                          dataset_desc='resnext50_reimplementation_pikachu_dataset', 
                                                                          val_loader=pikachu_val_loader,
                                                                          weight_decay=0.1, savedModel=True)

resnext50_loss_history, resnext50_acc_history = train(resnext50_model, pikachu_trainloader, num_epoch=10, 
                                                                          dataset_desc='resnext50_pikachu_dataset', 
                                                                          val_loader=pikachu_val_loader,
                                                                          weight_decay=0.1, savedModel=True)

resnet18_loss_history, resnet18_acc_history = train(resnet18_model, pikachu_trainloader, num_epoch=10, 
                                                                          dataset_desc='resnet18_pikachu_dataset', 
                                                                          val_loader=pikachu_val_loader,
                                                                          weight_decay=0.1, savedModel=True)

"""**Evaluate model**"""

evaluate(random_model, pikachu_testloader, printROC=True)

evaluate(resnext50_reimplement_model, pikachu_testloader, printROC=True)

evaluate(resnext50_model, pikachu_testloader, printROC=True)

evaluate(resnet18_model, pikachu_testloader, printROC=True)

"""**Plot train loss**"""

# Train Loss Graph
import matplotlib.pyplot as plt
plt.plot(resnext50_reimplement_loss_history["train"], label='Reimplement Resnext 50')
plt.plot(resnext50_loss_history["train"], label='Resnext 50')
plt.plot(resnet18_loss_history["train"], label='Resnet 18')
plt.plot(random_model_loss_history["train"], label='Random model')
plt.legend()
plt.show()

# Validation Loss Graph
plt.plot(resnext50_reimplement_loss_history["val"], label='Reimplement Resnext 50')
plt.plot(resnext50_loss_history["val"], label='Resnext 50')
plt.plot(resnet18_loss_history["val"], label='Resnet 18')
plt.plot(random_model_loss_history["val"], label='Random model')
plt.legend()
plt.show()

# Train Accuracy Graph
plt.plot(resnext50_reimplement_acc_history["train"], label='Reimplement Resnext 50')
plt.plot(resnext50_acc_history["train"], label='Resnext 50')
plt.plot(resnet18_acc_history["train"], label='Resnet 18')
plt.plot(random_model_acc_history["train"], label='Random model')
plt.legend()
plt.show()

# Validation Accuracy Graph
plt.plot(resnext50_reimplement_acc_history["val"], label='Reimplement Resnext 50')
plt.plot(resnext50_acc_history["val"], label='Resnext 50')
plt.plot(resnet18_acc_history["val"], label='Resnet 18')
plt.plot(random_model_acc_history["val"], label='Random model')
plt.legend()
plt.show()