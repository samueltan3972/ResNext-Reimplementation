# -*- coding: utf-8 -*-
"""CIFAR10 - ResNext Deep Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n4GX0WDr93yfG1svTMrge4DwHJEQSunx
"""

# from google.colab import drive
# drive.mount('/content/drive')

# cd "/content/drive/MyDrive/pikachu_dataset/"

import numpy as np
import matplotlib.pyplot as plt
import os

import torch, torchvision
import torch.nn as nn
import torch.nn.functional as F

import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.optim import lr_scheduler

import math
import json

if not os.path.exists("./models"):
      os.mkdir("models")

device = "cuda" if torch.cuda.is_available() else "cpu"

"""**Helper Function**"""

def train(model, trainloader, num_epoch=5, dataset_desc='', val_loader=None, lr=0.1, momentum=0.9, weight_decay=0.001, savedModel=False):
    
    # to store loss and accuracy stat
    model_stats = {}

    accuracy_stats = {
        'train': [],
        "val": []
    }

    loss_stats = {
        'train': [],
        "val": []
    }

    # compute loss 3 times in each epoch
    loss_iterations = int(np.ceil(len(trainloader)/3))
    stagnant_count = 0
    best_loss = math.inf

    # transfer model to GPU
    model = model.to(device)

    # set the optimizer
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)

    # set the scheduler
    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    # init first epoch
    previous_epoch = -1

    # prepare to load checkpoint file
    if savedModel:
      path = './models/' + dataset_desc

      # load the checkpoint file
      if not os.path.exists(path):
        os.mkdir(path)

      model_files = [ f  for f in os.listdir(path) if f.endswith('.pt')]
      model_files.sort(reverse=True)

      # load if only exist checkpoint file
      if model_files:
        try: 
          checkpoint = torch.load(path + '/' + model_files[0])      
          model.load_state_dict(checkpoint['model_state_dict'])
          optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
          previous_epoch = checkpoint['epoch']
          previous_train_loss = checkpoint['train_loss']
          previous_val_loss = checkpoint['val_loss']

          print(f'Resuming previous epoch. Last run epoch: {previous_epoch}, last train loss: {previous_train_loss}, last val loss: {previous_val_loss}')
        except: 
          print("Error in saved model")    
      
      # load the savedModel stat file
      stat_file_name = path + '/' + dataset_desc + '_stat.json'
      
      if os.path.isfile(stat_file_name):
        with open(stat_file_name, 'r') as f:
          try:
            model_stats = json.load(f)
            accuracy_stats = model_stats["accuracy"]
            loss_stats = model_stats["loss"]
          except:
            print("Error in saved model stat")   

    # train the network
    for e in range(previous_epoch+1, previous_epoch+1+num_epoch):    

        # set to training mode
        model.train()

        running_train_loss = 0
        running_train_count = 0
        running_train_corrects = 0
        
        for i, (train_inputs, train_labels) in enumerate(trainloader):
            # Clear all the gradient to 0
            optimizer.zero_grad()

            # transfer data to GPU
            train_inputs = train_inputs.to(device)
            train_labels = train_labels.to(device)

            # forward propagation to get h
            train_outs = model(train_inputs)

            # compute train loss 
            train_loss = F.cross_entropy(train_outs, train_labels)

            # compute train accuracy
            _, predicted = torch.max(train_outs, 1)
            running_train_corrects += (train_labels == predicted).double().sum()

            # backpropagation to get gradients of all parameters
            train_loss.backward()

            # update parameters
            optimizer.step()

            # get the loss
            running_train_loss += train_loss.item()
            running_train_count += 1
            
            # display the averaged loss value (training)
            if i % loss_iterations == loss_iterations-1 or i == len(trainloader) - 1:    
                # compute training loss
                train_loss = running_train_loss / running_train_count
                running_train_loss = 0. 
                running_train_count = 0.
               
                print(f'[Epoch {e+1:2d} Iter {i+1:5d}/{len(trainloader)}]: train_loss = {train_loss:.4f}')       

                # track train loss
                loss_stats['train'].append(train_loss)

                # to stop the training if the result is not improving
                if train_loss >= best_loss:
                  stagnant_count += 1
                else:
                  best_loss = train_loss
                  stagnant_count = 0

                if stagnant_count >= 3:
                  break
              
        # track train accuracy
        train_accuracy = 100*running_train_corrects/len(trainloader.dataset)
        accuracy_stats['train'].append(train_accuracy.item())
        
        # print train accuracy
        print(f'[Epoch {e+1:2d}: train_accuracy = {train_accuracy:.2f}%')   

        # init val loss
        val_loss = math.inf

        # VALIDATION
        if val_loader is not None:
          with torch.no_grad():

              # set to evaluation mode
              model.eval()

              running_val_loss = 0
              running_val_count = 0
              running_val_corrects = 0

              for i, (val_inputs, val_labels) in enumerate(val_loader):

                  # transfer data to GPU
                  val_inputs = val_inputs.to(device)
                  val_labels = val_labels.to(device)

                  # prediction with validation set
                  val_outs = model(val_inputs)

                  # compute loss 
                  val_loss = F.cross_entropy(val_outs, val_labels)

                  # compute validation accuracy
                  _, predicted = torch.max(val_outs, 1)
                  running_val_corrects += (val_labels == predicted).double().sum()

                  # get loss
                  running_val_loss += val_loss.item()
                  running_val_count += 1
                  
                  # display the averaged loss value 
                  if i % loss_iterations == loss_iterations-1 or i == len(val_loader) - 1:    
                      # compute training loss
                      val_loss = running_val_loss / running_val_count
                      running_val_loss = 0. 
                      running_val_count = 0.
                    
                      print(f'[Epoch {e+1:2d} Iter {i+1:5d}/{len(val_loader)}]: val_loss = {val_loss:.4f}')       

                      # track validation loss
                      loss_stats['val'].append(val_loss)
                  
          # track validation accuracy
          val_accuracy = 100*running_val_corrects/len(val_loader.dataset)
          accuracy_stats['val'].append(val_accuracy.item())

          # print validation accuracy
          print(f'[Epoch {e+1:2d}: val_accuracy = {val_accuracy:.2f}%')   

        # Update the scheduler's counter at the end of each epoch
        scheduler.step()

        # stop the training if no improvement
        if stagnant_count >= 3:
          return loss_stats, accuracy_stats

        # save the model 
        if savedModel:
          checkpoint_file = './models/' + dataset_desc + '/' + dataset_desc + '_model_epoch_'+ str(e) + '.pt'

          torch.save({
              'epoch': e,
              'train_loss': train_loss,
              'val_loss': val_loss,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict()
          }, checkpoint_file)

          # save the model stat
          stat_file_name = './models/' + dataset_desc + '/' + dataset_desc + '_stat.json'

          with open(stat_file_name, 'w') as f:
              model_stats["accuracy"] = accuracy_stats
              model_stats["loss"] = loss_stats
              json.dump(model_stats, f, indent=2)

    return loss_stats, accuracy_stats

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, RocCurveDisplay

def evaluate(net, testloader, printROC=False):
    
    # set to evaluation mode
    net.eval() 

    # running_correct
    running_corrects = 0
    y_pred = []
    y_true = []
    y_scores = []

    for inputs, targets in testloader:

        # transfer to the GPU
        inputs = inputs.to(device)
        targets = targets.to(device)

        # perform prediction (no need to compute gradient)
        with torch.no_grad():
            outputs = net(inputs)  
            _, predicted = torch.max(outputs, 1)

            running_corrects += (targets == predicted).double().sum()
            y_scores.extend(_.cpu().numpy())        # Save prediction scores
            y_pred.extend(predicted.cpu().numpy())  # Save prediction
            y_true.extend(targets.cpu().numpy())    # Save Truth
        
    print('Accuracy = {:.2f}%'.format(100*running_corrects/len(testloader.dataset)))

    # Plot Confusion Matrix
    conf_matrix = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=testloader.dataset.classes)
    disp.plot()
    plt.title('Confusion Matrix')
    plt.show() 

    # Plot ROC Curve and AUC value
    if printROC:
      fpr, tpr, thresholds = roc_curve(y_true, y_scores)
      roc_auc = auc(fpr, tpr)
      display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='ROC Curve')
      display.plot()
      plt.title('ROC Curve')
      plt.show()

"""**ResNext Model** 

Will be compare with resnext (reimplement)
"""

def conv1x1(input, output, stride=1):
  return nn.Conv2d(input, output, kernel_size=1, stride=stride, bias=False)

def conv3x3(input, output, groups=1, padding=0, stride=1):
  return nn.Conv2d(input, output, kernel_size=3, groups=groups, padding=padding, stride=stride, bias=False)

class Bottleneck(nn.Module):
  def __init__(self, in_channel, mid_channel, out_channel, stride=1, downsample = None):
    super().__init__()

    self.conv1 = conv1x1(in_channel, mid_channel)
    self.bn1 = nn.BatchNorm2d(mid_channel)
    self.conv2 = conv3x3(mid_channel, mid_channel, groups=32, padding=1, stride=stride)
    self.bn2 = nn.BatchNorm2d(mid_channel)
    self.conv3 = conv1x1(mid_channel, out_channel)
    self.bn3 = nn.BatchNorm2d(out_channel)
    self.relu = nn.ReLU(inplace=True)
    self.downsample = downsample

  def forward(self, x):
    identity = x

    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)

    out = self.conv2(out)
    out = self.bn2(out)
    out = self.relu(out)

    out = self.conv3(out)
    out = self.bn3(out)

    if self.downsample is not None:
      identity = self.downsample(x)

    out += identity  # skip connection
    out = self.relu(out)

    return out

# Resnext reimplement for CIFAR 10, followed the paper
class resnext_reimplement(nn.Module):
  def __init__(self, num_classes=10):
    super().__init__()
    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=3, stride=2, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, dilation=1)
    self.layer1 = self._make_layer(1)
    self.layer2 = self._make_layer(2)
    self.layer3 = self._make_layer(3)
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear(1024, num_classes)
    self.softmax = nn.Softmax(dim=1)

    for m in self.modules():
      if isinstance(m, (nn.Conv2d, nn.Linear)):
          nn.init.kaiming_normal_(m.weight)
      elif isinstance(m, (nn.BatchNorm2d)):
          nn.init.ones_(m.weight)
          nn.init.zeros_(m.bias)

  def _make_layer(self, layer_num):
    layer_sizes = [3, 3, 3]
    channel_sizes = [(64, 64, 256), (256, 256, 512), (512, 512, 1024)]

    # init stride for layer
    if layer_num == 1:
      stride = 1
    else:
      stride = 2

    # to fit to list position
    layer_num -=  1

    # downsample layer for residual connection
    downsample = nn.Sequential(
      conv1x1(channel_sizes[layer_num][0], channel_sizes[layer_num][2], stride=stride),
      nn.BatchNorm2d(channel_sizes[layer_num][2])
    )
    
    # generate first layer in each layer blocks
    layers = []
    layers.append(
        Bottleneck(channel_sizes[layer_num][0], channel_sizes[layer_num][1], channel_sizes[layer_num][2], stride=stride, downsample=downsample)
    )

    # generate the rest of the layers
    for _ in range(1, layer_sizes[layer_num]):
      layers.append(
          Bottleneck(channel_sizes[layer_num][2], channel_sizes[layer_num][1], channel_sizes[layer_num][2])
      )

    # return the layer block
    return nn.Sequential(*layers)

  def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.maxpool(x)

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)

    x = self.avgpool(x)
    x = torch.flatten(x, 1)
    x = self.fc(x)

    #x = self.softmax(x)

    return x

"""**Load Dataset**

Load CIFAR 10 to test and build the model first, custom dataset will be later
"""

import numpy as np
import torchvision
from torch.utils.data import Dataset

class CIFAR10(Dataset):
    
    def __init__(self, root = ".", train = True, download=False, transform=None, num_samples = None):
        self.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
        self.num_classes = 10          

        self.__dataset = torchvision.datasets.CIFAR10(root=root, 
                                                      train=train, 
                                                      download=download,
                                                      transform=transform)

        if num_samples is not None:
            self.__num_samples = min(num_samples, len(self.__dataset))
        else:
            self.__num_samples = len(self.__dataset)
                               
    def __len__(self):
        return self.__num_samples
    
    def __getitem__(self, idx):            
        return self.__dataset[idx]

# transform the model
transform = transforms.Compose([
 transforms.RandomHorizontalFlip(),
 transforms.ToTensor(),
 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# dataset
cifar10_trainset = CIFAR10(train=True, transform=transform, num_samples=5000, download=True)
cifar10_testset = CIFAR10(train=False, transform=transform, num_samples=1000, download=True)

# Lower down the batch size if GPU not enough
cifar10_trainloader = DataLoader(cifar10_trainset, batch_size=64, shuffle=True, num_workers=2)
cifar10_testloader = DataLoader(cifar10_testset, batch_size=10, shuffle=True, num_workers=2)

print(len(cifar10_trainset))

"""**Init model**"""

# define reimplement model
resnext_reimplement_model = resnext_reimplement(num_classes=10)

import torchvision.models as models

# build the network
resnet18_model = models.resnet18()
in_c = resnet18_model.fc.in_features
resnet18_model.fc = nn.Linear(in_c, 10)

"""**TensorBoard**"""

from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/cifar10_model')

inputs =torch.randn(4, 3, 224, 224)
writer.add_graph(resnext_reimplement_model, inputs)

writer.close()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir runs
#%reload_ext tensorboard

"""**Inspect model / View model**

Change the last fc layer to specific number
"""

print(resnext_reimplement_model)

"""**Train model**"""

resnext_reimplement_loss_history, resnext_reimplement_acc_history = train(resnext_reimplement_model, cifar10_trainloader, num_epoch=20, 
                                                                          dataset_desc='cifar10_dataset', 
                                                                          weight_decay=0.1, savedModel=True)

resnet18_loss_history, resnet18_acc_history = train(resnet18_model, cifar10_trainloader, num_epoch=1, 
                                                                dataset_desc='resnet_cifar10_dataset', 
                                                                weight_decay=0.1, savedModel=True)

"""**Evaluate model**"""

evaluate(resnext_reimplement_model, cifar10_testloader)

evaluate(resnet18_model, cifar10_testloader)

"""**Plot Graph**"""

# Train Loss Graph
import matplotlib.pyplot as plt
plt.plot(resnext_reimplement_loss_history["train"], label='Reimplement Resnext 50')
plt.plot(resnet18_loss_history["train"], label='Resnet 18')
plt.legend()
plt.show()

# Train Accuracy Graph
plt.plot(resnext_reimplement_acc_history["train"], label='Reimplement Resnext 50')
plt.plot(resnet18_acc_history["train"], label='Resnet 18')
plt.legend()
plt.show()